{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import gzip\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data heavy preprocessing\n",
    "\n",
    "This section processes the raw data and write it in a format which is directly usable in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "        \n",
    "def parseUncompressed(path):\n",
    "    g = open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "def metadata_for_category(path, category):\n",
    "    \"\"\"\n",
    "    Reads the metadata file and extract only metadata for the given category\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parseUncompressed(path):\n",
    "        if 'categories' in d:\n",
    "            for categories_list in d['categories']:\n",
    "                if categories_list[0] == category:\n",
    "                    df[i] = d\n",
    "                    i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "def readSerialized(path):\n",
    "    return pd.read_pickle(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df = metadata_for_category('data/metadata.json', 'Electronics')\n",
    "#df.to_pickle('data/metadata_electronics_serialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data (light) preprocessing\n",
    "\n",
    "* df stores the raw review data for electronics\n",
    "* metadata stores the metadata for electronics\n",
    "* data stores the merged dataframe between df and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = readSerialized('data/serialized_electronics')\n",
    "metadata = readSerialized('data/metadata_electronics_serialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep only metadata for known brands\n",
    "metadata = metadata[metadata.brand.notnull()]\n",
    "\n",
    "# Remove columns which are not needed for our analysis\n",
    "metadata = metadata.drop(['related', 'categories'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = df.merge(metadata, how='left', left_on='asin', right_on='asin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of items: \", len(data))\n",
    "print(\"Number of items with metadata: \", len(data[data.title.notnull()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opinion extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getScore(x):\n",
    "    if(len(x) == 2 and  x[1] > 2):\n",
    "        return x[0] / x[1]\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "df2['reviewLength'] = df2['reviewText'].str.len()\n",
    "df2['score'] = df2.apply(lambda x : getScore(x['helpful']), axis=1)\n",
    "df2 = df2[['asin','score','reviewLength']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df2.sort_values(['score','reviewLength'],ascending=False)\n",
    "display(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import bootstrap_plot\n",
    "\n",
    "dfplot = df2[['score','reviewLength']].sample(n=50000)\n",
    "\n",
    "dfplot.plot.scatter(y = 'score',x ='reviewLength',style=['o', 'rx'], s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df.groupby([\"asin\"])['reviewText'].agg(lambda x:''.join(set(x))).reset_index()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filterTags(w1,w2):\n",
    "    _, tag1 = nltk.pos_tag(nltk.word_tokenize(w1))[0]\n",
    "    _, tag2 = nltk.pos_tag(nltk.word_tokenize(w2))[0]\n",
    "    \n",
    "    return (tag1.startswith('JJ') and tag2.startswith('NN')) or \\ # Good quality\n",
    "            (tag1.startswith('RB') and tag2.startswith('VBN')) or \\ # Well made\n",
    "            (tag1.startswith('VB') and tag2.startswith(\"JJ\")) # Work well\n",
    "    \n",
    "def getBest(text):\n",
    "    ignored_words = nltk.corpus.stopwords.words('english')\n",
    "    word_filter = lambda w: len(w) < 3 or w.lower() in ignored_words\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    \n",
    "    finder = nltk.BigramCollocationFinder.from_words(tokens)\n",
    "    finder.apply_word_filter(word_filter)    \n",
    "    finder.apply_freq_filter(3)\n",
    "    res = finder.ngram_fd.most_common(3)\n",
    "        \n",
    "    res = [x for x in res if filterTags(x[0][0],x[0][1])]\n",
    "\n",
    "    if(len(res) > 0):\n",
    "        return res\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "dfProduct = df.groupby([\"asin\"])['reviewText'].agg(lambda x:''.join(set(x))).reset_index()\n",
    "dfProduct = dfProduct.sample(n=2000)\n",
    "\n",
    "start = timer()\n",
    "dfProduct[\"reviewText\"] = dfProduct[\"reviewText\"].apply(lambda x: getBest(x))\n",
    "end = timer()\n",
    "print(end - start)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfProduct = dfProduct.dropna(how = 'any')\n",
    "dfProduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df2['Best'] = df2['reviewText'].apply(lambda x: x[0][0])\n",
    "#df2['Second'] = df2['reviewText'].apply(lambda x: x[1][0])\n",
    "#df2['Third'] = df2['reviewText'].apply(lambda x: x[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentimentAnalyser:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "    def _penn_to_wn(self, tag):\n",
    "        \"\"\"\n",
    "        Convert between the PennTreebank tags to simple Wordnet tags\n",
    "        \"\"\"\n",
    "        if tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        elif tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        return None\n",
    "\n",
    "    def sentiment_for_tagged_word(self, tagged_word):\n",
    "        \"\"\"\n",
    "        Compute the score for a given tagged word.\n",
    "        The word is assumed to be tagged using the Penn Treebank Project's tags\n",
    "        Return None for irrelevant words, a tuple (positive score, negative score) otherwise\n",
    "        \"\"\"\n",
    "        word, tag = tagged_word\n",
    "        \n",
    "        wn_tag = self._penn_to_wn(tag)\n",
    "        \n",
    "        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "            return None\n",
    "        \n",
    "        lemma = self.lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        \n",
    "        if not lemma:\n",
    "            return None\n",
    "        \n",
    "        synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "        \n",
    "        if not synsets:\n",
    "            return None\n",
    "        \n",
    "        synset = synsets[0]\n",
    "        swn_synset = swn.senti_synset(synset.name())\n",
    "        \n",
    "        return swn_synset.pos_score(), swn_synset.neg_score()\n",
    "    \n",
    "    def sentiment_score_for_raw_sentence(self, raw_sentence):\n",
    "        \"\"\"\n",
    "        Compute the sum of the differences in sentiment score for each word in the sentence\n",
    "        \"\"\"\n",
    "        tagged_sentence = nltk.pos_tag(word_tokenize(raw_sentence))\n",
    "        sum_deltas = 0\n",
    "\n",
    "        for tagged_word in tagged_sentence:\n",
    "            scores = self.sentiment_for_tagged_word(tagged_word)\n",
    "\n",
    "            if scores is None:\n",
    "                continue\n",
    "                \n",
    "            pos_score, neg_score = scores\n",
    "            sum_deltas += (pos_score - neg_score)\n",
    "        \n",
    "        return sum_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = SentimentAnalyser()\n",
    "a = s.sentiment_score_for_raw_sentence(\"low price\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_sentence = \"Dankest object in the whole fucking world\"\n",
    "tagged_sentence = nltk.pos_tag(word_tokenize(raw_sentence))\n",
    "\n",
    "for word, tag in tagged_sentence:\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "        continue\n",
    "\n",
    "    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "    if not lemma:\n",
    "        continue\n",
    "\n",
    "    synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "    if not synsets:\n",
    "        continue\n",
    "\n",
    "    # Take the first sense, the most common\n",
    "    synset = synsets[0]\n",
    "    swn_synset = swn.senti_synset(synset.name())\n",
    "\n",
    "    print(swn_synset)\n",
    "    print(swn_synset.pos_score())\n",
    "    print(swn_synset.neg_score())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "sentences=[\"low price\",\"the option to pass keyword arguments to the underlying matplotlib plotting method.\"]\n",
    "sid = SIA()\n",
    "for sentence in sentences:\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data = pd.read_csv(\"\")\n",
    "\n",
    "import random\n",
    " \n",
    "sentiment_data = list(zip(df[\"reviewText\"], df[\"overall\"]))\n",
    "random.shuffle(sentiment_data)\n",
    " \n",
    "l = 200000    \n",
    "\n",
    "# 80% for training\n",
    "train_X, train_y = zip(*sentiment_data[:l])\n",
    " \n",
    "# Keep 20% for testing\n",
    "test_X, test_y = zip(*sentiment_data[l:l+int(l/10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "bigram_clf = Pipeline([\n",
    "('vectorizer', CountVectorizer(analyzer=\"word\",\n",
    "ngram_range=(2, 2),\n",
    "tokenizer=word_tokenize, \n",
    "# tokenizer=lambda text: mark_negation(word_tokenize(text)),\n",
    "preprocessor=lambda text: text.replace(\"<br />\", \" \"),)),\n",
    "('classifier', LinearSVC(verbose=True))\n",
    "])\n",
    "bigram_clf.fit(train_X, train_y)\n",
    "bigram_clf.score(test_X, test_y)\n",
    "# with mark_negation 0.86760000000000004\n",
    "# without mark_negation 0.87119999999999997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_clf.predict([\"bad laptop\"])[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
