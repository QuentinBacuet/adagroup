{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import gzip\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "import random \n",
    "from matplotlib import pyplot\n",
    "from pylab import rcParams\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "rcParams['figure.figsize'] = 15, 10\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we will explore opinion mining and sentiment analysis through the use of natural language processing. Our dataset consists of reviews of electronic products scraped form Amazon and available here: http://jmcauley.ucsd.edu/data/amazon/\n",
    "\n",
    "The objective of this analysis is to use the reviews to extract meaningful characteristics about their respective product. Such characteristics can then be used to help the user make faster and more informed decisions when shopping on electronic commerce websites. The extracted characteristics consist of pairs of words that describe a property of the product such as its quality or performance. Those pairs are either of the type adjective-noun or adverb-past participle.\n",
    "\n",
    "For example:\n",
    "\n",
    "    Adjective - Nouns: (good, quality) (low, price) (best, deal)\n",
    "    Adverb - Past Participle pairs such as (well, made) (poorly, assembled)\n",
    "\n",
    "The characteristics can then be categorised into positive and negative opinions and shown to the subsequent potential buyers. This way, users can quickly be informed on the general trends and opinions formulated by previous clients about a particular product. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<ol>\n",
    "<li> <a href=\"#Data-Overview\">Data Overview</a>\n",
    "    <ol>\n",
    "        <li><a href=\"#Reading-the-data\">Reading the data</a></li>\n",
    "        <li><a href=\"#Data-Overview\">Formats</a></li>\n",
    "        <li><a href=\"#Missing-values\">Missing values</a></li>\n",
    "        <li><a href=\"#Distributions\">Distributions</a></li>\n",
    "    </ol>\n",
    "</li>\n",
    "<p></p>\n",
    "<li> <a href=\"#Characteristics-Extraction\">Characteristics Extraction</a></li>\n",
    "<p></p>\n",
    "<li><a href=\"#Sentiment-Analysis\">Sentiment Analysis</a></li>\n",
    "<p></p>\n",
    "        <li><a href=\"#Sentiment-Analysis,-revisited\">Sentiment Analysis, revisited</a></li>\n",
    "<p></p>\n",
    "<li><a href=\"#Application-to-brand-names\">Application to brand names</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data \n",
    "**<a href=\"#Table-of-Contents\">Back to table of contents</a>**\n",
    "\n",
    "For this intermediate milestone we will be using the following datasets:\n",
    "\n",
    "1. Sample of electronic review (1,689,188 reviews)\n",
    "2. Full dataset metadata\n",
    "\n",
    "#### 1. Electronic reviews sample\n",
    "Those easily fit in memory. Therefore, we can read the data line by line and store the result in a dataframe.\n",
    "#### 2. Full dataset metadata\n",
    "The metadata was downloaded from the cluster. It is not possible to load it in memory as when uncompressed it is more than 10GB in size. Since we care about electronic products for the moment, we read it line by line and only store the entries whose category is related to electronics. This results in a much smaller file of approx 500Mo (uncompressed) which can hold in memory.\n",
    "\n",
    "## Scalability\n",
    "We argue that processing the data locally will scale reasonably well to the rest of the Electronic reviews as our pipeline is applied separately for each product. That is, we only need to hold the reviews of one product as well as its metadata in memory at any given time.\n",
    "\n",
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data file\n",
    "To open the data file we reuse the python code given by the author of the dataset. (See http://jmcauley.ucsd.edu/data/amazon/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "        \n",
    "def parseUncompressed(path):\n",
    "    g = open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    \"\"\"\n",
    "    Reads a data file and use it the build a DataFrame\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metadata_for_category(path, category):\n",
    "    \"\"\"\n",
    "    Reads the metadata file and extract only metadata for the given category\n",
    "    path: The path to the metadata file\n",
    "    category: The product category for which to extract the metadata\n",
    "    \n",
    "    Returns a DataFrame holding the metadata\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parseUncompressed(path):\n",
    "        if 'categories' in d:\n",
    "            for categories_list in d['categories']:\n",
    "                if categories_list[0] == category:\n",
    "                    df[i] = d\n",
    "                    i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and storing intermediate results\n",
    "DataFrames' `to_pickle` and `read_pickle` are used to respectively save and load serialized version of our intermediate results. We use it for example to store only the Electronics metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metadata_for_category' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9511ae38b8b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetadata_for_category\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/metadata.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Electronics'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/metadata_electronics_serialized.pickle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'metadata_for_category' is not defined"
     ]
    }
   ],
   "source": [
    "df = metadata_for_category('data/metadata.json', 'Electronics')\n",
    "df.to_pickle('data/metadata_electronics_serialized.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = getDF('data/reviews_Electronics_5.json.gz')\n",
    "df.to_pickle('data/electronics_serialized.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview\n",
    "**<a href=\"#Table-of-Contents\">Back to table of contents</a>**\n",
    "\n",
    "We start by performing some basic statistical analysis on our data. By doing this, we seek to find if there are any unusual patterns which could be interesting in the context of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_elec = pd.read_pickle('data/electronics_serialized.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_meta = pd.read_pickle('data/metadata_electronics_serialized.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now describe the different formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall                  float64\n",
       "reviewTime        datetime64[ns]\n",
       "reviewerID                object\n",
       "reviewText                object\n",
       "summary                   object\n",
       "unixReviewTime             int64\n",
       "asin                      object\n",
       "reviewerName              object\n",
       "helpful                   object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_elec['reviewTime']  = pd.to_datetime(df_elec['reviewTime'],format='%m %d, %Y')\n",
    "df_elec.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* summary: The summary of the product\n",
    "* reviewerName: The reviewer name\n",
    "* reviewTime: The review time in the datetime type\n",
    "* overall: The rating of the product from 1 to 5 included\n",
    "* asin: The id of the product\n",
    "* helpfull: The helpfulness of the comment. It is stored as an array; the first element is the number of positive votes the second element is the total number of votes for the comment.\n",
    "* unixReviewTime: The review time in the unix format\n",
    "* reviewerID: The reviewer id\n",
    "* reviewText: The text of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_meta = df_meta[pd.notnull(df_meta['brand'])]\n",
    "df_meta.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* title: The name of the product\n",
    "* description: The description of the product\n",
    "* imUrl: The url of the image\n",
    "* categories: The categories of the product, stored as an array\n",
    "* asin: The id of the product\n",
    "* price: The price of the product\n",
    "* selesRank: The sales rank, stored as dictionary mapping the category to the rank\n",
    "* related: The related products, stored as dictionary mapping a tag (also_bought, bought_together,...) to the asin\n",
    "* brand: The name of the brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(df_elec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "**<a href=\"#Table-of-Contents\">Back to table of contents</a>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the data, we discarded every null entries in price and reviewLength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_merged = df_meta.merge(df_elec, left_on='asin', right_on='asin', how='inner')\n",
    "df_merged['reviewLength'] = df_merged['reviewText'].str.len()\n",
    "\n",
    "df_plot = df_merged.copy()\n",
    "df_plot = df_plot[pd.notnull(df_plot['price'])]\n",
    "df_plot = df_plot[pd.notnull(df_plot['reviewLength'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions\n",
    "**<a href=\"#Table-of-Contents\">Back to table of contents</a>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plotted the scatter matrix of the price, the review length and the overall score, to have some insight on the global distribution of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(df_plot[['price','reviewLength','overall']], alpha=0.05, figsize=(15, 15), diagonal='hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also plotted the price over the review length to see in more details the data. We can directly see that there is a lot of noise (as expected). We also see that most of the mass is concentrated in a rather small interval; indeed the median of the price is 28.5 and the median of the text length is 312."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.regplot(y='reviewLength', x='price', data=df_plot,scatter_kws={'s':5},line_kws={'color':'r'},order=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also plotted the distributions of the reviewLength over the overall rating. We can see that the extremes are less present in the data. Indeed, the review length is smaller for good reviews, this can be explained by the fact those reviews are synthetic: “Well made”, “Good Product”. The reviews that have a bad score (2) tend to be larger, as the critic can be quite extensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(y='overall', x='reviewLength', data=df_plot, orient='h', showfliers=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we sought to understand the distribution of the price and the review length by plotting their density and the box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "sns.boxplot(data= df_plot[['price']],orient='h', showfliers=False,ax=axes[0,0])\n",
    "df_plot[['price']].plot.kde(ax=axes[0,1])\n",
    "sns.boxplot(data= df_plot[['reviewLength']],orient='h', showfliers=False,ax=axes[1,0])\n",
    "df_plot[['reviewLength']].plot.kde(ax=axes[1,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to fit a specific distribution; the exponential distribution to the data provided. We see that if we pick lambda = 65, we get a close approximation to the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "import scipy.stats\n",
    "\n",
    "d = df_plot[['price']]\n",
    "ax = d.plot.kde()\n",
    "x = ax.get_children()[0]._x\n",
    "y = ax.get_children()[0]._y\n",
    "\n",
    "scale = 66\n",
    "loc = -480\n",
    "\n",
    "pdf_fitted = expon.pdf(x - loc, scale=scale)\n",
    "plt.plot(pdf_fitted,label= ('exp lambda = ' + str(scale)))\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to fit a specific distribution; the pareto distribution to the data provided. We see that if we pick b = 1, we get a close approximation to the result up to a multiplicative constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pareto\n",
    "import scipy.stats\n",
    "\n",
    "d = df_plot[['reviewLength']]\n",
    "ax = d.plot.kde()\n",
    "\n",
    "b = 1\n",
    "x = np.linspace(pareto.ppf(0.01, b),pareto.ppf(0.99, b), 50000)\n",
    "pdf_fitted = scipy.stats.pareto.pdf(x,b = b)/500\n",
    "plt.plot(pdf_fitted,label= 'pareto')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We computed the correlation between the price, reviewLength, and the overall. We can see that the overall doesn't seem to be a lot correlated between the price and the review length. But more interestingly there seem to be some correlation between the price and the review lenght. Maybe the user are more passionate about the items they are willing to spend the more money on !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_merged[['price','reviewLength','overall']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characteristics Extraction\n",
    "**<a href=\"#Table-of-Contents\">Back to table of contents</a>**\n",
    "\n",
    "Now that we've acquired basic insight on our data, we will start describing the core of our project. We want to be able to extract opinions and product characteristics from reviews. One simple and effective way of doing that is to find adjective-nouns pairs that happen several times for a given product. Such pairs of words are also called collocations. Collocations are tuples of words that happen more frequently than one would expect if the words were distributed randomly in the text.\n",
    "\n",
    "To find collocations, we use the [Natual Language Tool Kit (NLTK)](http://www.nltk.org/) package for Python. NLTK also provides an interface for the [WordNet](https://wordnet.princeton.edu/) lexical database.\n",
    "\n",
    "WordNet can be used to find lexical relations between words. It is also capable of returning a part-of-speech (POS) tags for any given word. The nomenclature chosen for those tags is the open from the [Penn Treebank Project](https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf). The tags used by our pipeline are the following:\n",
    "\n",
    "Every tag starting with *JJ*: \n",
    "* JJ -\tAdjective\n",
    "* JJR -\tAdjective, comparative\n",
    "* JJS -\tAdjective, superlative \n",
    "\n",
    "Every tag starting with *RB*:\n",
    "* RB -\tAdverb\n",
    "* RBR - Adverb, comparative\n",
    "* RBS - Adverb, superlative \n",
    "* VBN -\tVerb, past participle \n",
    "\n",
    "Every tag starting with *NN*:\n",
    "* NN -\tNoun, singular or mass\n",
    "* NNS -\tNoun, plural\n",
    "* NNP -\tProper noun, singular\n",
    "* NNPS - Proper noun, plural \n",
    "\n",
    "Every tag starting with *VB*\n",
    "* VB -\tVerb, base form\n",
    "* VBD -\tVerb, past tense\n",
    "* VBG -\tVerb, gerund or present participle\n",
    "* VBN -\tVerb, past participle\n",
    "* VBP -\tVerb, non-3rd person singular present\n",
    "* VBZ -\tVerb, 3rd person singular present \n",
    "\n",
    "As explained in the introduction, we will extract pairs consisting of Adjective-Nouns (like \"good quality\"), Adverb - Past Participle (like \"well made\") and verb-adverb (like \"works well\"). To do this, we regroup reviews by product and concatenate them. Each product with its reviews can then be feed to the collocations finder. \n",
    "\n",
    "We start by defining the required functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterTags(w1,w2):\n",
    "    \"\"\"\n",
    "    w1: First tagged word\n",
    "    w2: Second tagged word\n",
    "    \n",
    "    Returns true if the bigram collocation is of type adjective-noun, adverb-past participle or verb-adverb\n",
    "    \"\"\"\n",
    "    _, tag1 = nltk.pos_tag(nltk.word_tokenize(w1))[0]\n",
    "    _, tag2 = nltk.pos_tag(nltk.word_tokenize(w2))[0]\n",
    "    \n",
    "    return  (tag1.startswith('JJ') and tag2.startswith('NN')) or \\\n",
    "            (tag1.startswith('RB') and tag2.startswith('VBN')) or \\\n",
    "            (tag1.startswith('VB') and tag2==\"JJ\")\n",
    "        \n",
    "def isImportantTag(tag):\n",
    "    return tag[:2] in ['JJ', 'NN', 'RB', 'VBN', 'VB', 'JJ']\n",
    "        \n",
    "def filterTrigramTags(w1, w2, w3):\n",
    "    \"\"\"\n",
    "    w1: First tagged word\n",
    "    w2: Second tagged word\n",
    "    w3: Third tagged word\n",
    "    \n",
    "    Returns true if each word in the trigram collocations is either an adjective, an adverb or a noun\n",
    "    \"\"\"\n",
    "    \n",
    "    _, tag1 = nltk.pos_tag(nltk.word_tokenize(w1))[0]\n",
    "    _, tag2 = nltk.pos_tag(nltk.word_tokenize(w2))[0]\n",
    "    _, tag3 = nltk.pos_tag(nltk.word_tokenize(w3))[0]\n",
    "    \n",
    "    return isImportantTag(tag1) and isImportantTag(tag2) and isImportantTag(tag3)\n",
    "\n",
    "\n",
    "def bigram_correlations(finder):\n",
    "    pass\n",
    "    \n",
    "\n",
    "import re, string\n",
    "\n",
    "def getCollocations(text, window=2,  find_trigrams=False):\n",
    "    \"\"\"\n",
    "    text: The concatenation of all the review for a given product\n",
    "    \n",
    "    Returns the collocations of words larger \n",
    "    \"\"\"\n",
    "    ignored_words = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    brigram_res = []\n",
    "    trigram_res = []\n",
    "    \n",
    "    pattern = re.compile('[\\W_]+')\n",
    "    text = pattern.sub('', text)\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "    \n",
    "    bigram_finder = nltk.BigramCollocationFinder.from_words(tokens, window_size=window)\n",
    "    \n",
    "    # Ignore bigram that are infrequent\n",
    "    bigram_finder.apply_freq_filter(3) \n",
    "    # Retrieves the 10 most common bigrams\n",
    "    bigram_res = bigram_finder.nbest(bigram_measures.pmi, 10) \n",
    "    bigram_res = [(x,round(bigram_finder.score_ngram(bigram_measures.pmi, x[0], x[1]),2)) \n",
    "                  for x in bigram_res if filterTags(x[0],x[1])]\n",
    "    \n",
    "    if find_trigrams:\n",
    "        trigram_finder = nltk.TrigramCollocationFinder.from_words(tokens)\n",
    "        trigram_finder.apply_freq_filter(3)\n",
    "        trigram_res = trigram_finder.nbest(trigram_measures.pmi, 10)\n",
    "        trigram_res = [(x, round(trigram_finder.score_ngram(trigram_measures.pmi, x[0], x[1], x[2]), 2))\n",
    "                  for x in trigram_res if filterTrigramTags(x[0], x[1], x[2])]\n",
    "    \n",
    "    if(len(bigram_res) > 0 or len(trigram_res) > 0):\n",
    "        return bigram_res + trigram_res\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And feed the reviews as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_product = df_elec.groupby([\"asin\"])['reviewText'].agg(lambda x:''.join(set(x.str.lower()))).reset_index()\n",
    "df_product = df_product.sample(n=100, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "  \n",
    "# Too slow if done sequentially\n",
    "start = timer()\n",
    "\n",
    "print(df_product[\"reviewText\"].apply(getCollocations, args=(2, True,)).dropna(how='any'))\n",
    "print(df_product[\"reviewText\"].apply(getCollocations, args=(3, True,)).dropna(how='any'))\n",
    "\n",
    "end = timer()\n",
    "print(end - start)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing pipeline performance\n",
    "The step of finding interesting collocations is by far the most computationnaly expensive and the bottleneck of our pipeline. However, there's a simple way of mitigating this. The key observation is that each product can be analyzed separately since we aggregate all reviews with their respective product. This means that we can parallelize the collocation finding algorithm over several products. Unfortunately, IPython notebooks do not support the use of multi-threaded code. We had to move it to the [ThreadedCollocations.py](ThreadedCollocations.py) file. The code there simply loads our dataset, setup a pool of 8 worker threads and call the function `getCollocations` on each product. Finally, it saves the resulting dataframe in a pickle file. The algorithm can parse all the 600k products (~1.6 million reviews)  in approximately 10 minutes on a laptop processor. We load the results from a pickle file below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collocations = pd.read_pickle(\"elec_collocations.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the results below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0528881469</td>\n",
       "      <td>[((basic, garmin), 7.05)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0972683275</td>\n",
       "      <td>[((retail, stores), 10.37), ((full, motion), 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1400501466</td>\n",
       "      <td>[((ad-supported, versions), 11.67), ((angry, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1400501776</td>\n",
       "      <td>[((full, android), 7.68)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1400532620</td>\n",
       "      <td>[((visual, feedback), 12.31), ((few, hours), 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1400532655</td>\n",
       "      <td>[((united, states), 13.03), ((reasonably, pric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>140053271X</td>\n",
       "      <td>[((virtual, keyboard), 11.22), ((previously, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1400532736</td>\n",
       "      <td>[((was, able), 7.77)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1400599997</td>\n",
       "      <td>[((real, world), 10.2)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3930992868</td>\n",
       "      <td>[((hard, drive), 5.71)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9575871979</td>\n",
       "      <td>[((full, charge), 6.97), ((tactical, flashligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>9983891212</td>\n",
       "      <td>[((flat, screen), 8.94), ((big, screen), 7.94)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>B000001OL6</td>\n",
       "      <td>[((normal, bias), 7.64), ((high, bias), 7.23)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>B00000DM9W</td>\n",
       "      <td>[((going, strong), 10.57), ((local, tv), 8.32)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>B00000J05A</td>\n",
       "      <td>[((technical, support), 8.74)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>B00000J061</td>\n",
       "      <td>[((internal, ferrite), 9.98)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>B00000J1SC</td>\n",
       "      <td>[((get, free), 8.0), ((different, colors), 7.67)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>B00000J1U8</td>\n",
       "      <td>[((good, quality), 6.26)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>B00000J1V3</td>\n",
       "      <td>[((high, quality), 7.17)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>B00000J1V5</td>\n",
       "      <td>[((sharp, bends), 10.51), ((am, happy), 8.8), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          asin                                         reviewText\n",
       "0   0528881469                          [((basic, garmin), 7.05)]\n",
       "3   0972683275  [((retail, stores), 10.37), ((full, motion), 1...\n",
       "4   1400501466  [((ad-supported, versions), 11.67), ((angry, b...\n",
       "6   1400501776                          [((full, android), 7.68)]\n",
       "7   1400532620  [((visual, feedback), 12.31), ((few, hours), 9...\n",
       "8   1400532655  [((united, states), 13.03), ((reasonably, pric...\n",
       "9   140053271X  [((virtual, keyboard), 11.22), ((previously, o...\n",
       "10  1400532736                              [((was, able), 7.77)]\n",
       "11  1400599997                            [((real, world), 10.2)]\n",
       "16  3930992868                            [((hard, drive), 5.71)]\n",
       "28  9575871979  [((full, charge), 6.97), ((tactical, flashligh...\n",
       "39  9983891212  [((flat, screen), 8.94), ((big, screen), 7.94)...\n",
       "44  B000001OL6     [((normal, bias), 7.64), ((high, bias), 7.23)]\n",
       "53  B00000DM9W    [((going, strong), 10.57), ((local, tv), 8.32)]\n",
       "58  B00000J05A                     [((technical, support), 8.74)]\n",
       "60  B00000J061                      [((internal, ferrite), 9.98)]\n",
       "76  B00000J1SC  [((get, free), 8.0), ((different, colors), 7.67)]\n",
       "78  B00000J1U8                          [((good, quality), 6.26)]\n",
       "83  B00000J1V3                          [((high, quality), 7.17)]\n",
       "84  B00000J1V5  [((sharp, bends), 10.51), ((am, happy), 8.8), ..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_product['reviewText'] = collocations['reviewText']\n",
    "df_product = df_product.dropna(how = 'any')\n",
    "df_product.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the characteristics extracted this way are already quite interesting. We see that some products were described several time as beig of \"high quality\" or \"well made\". However, we also note that some bigrams found do not actually correspond to product characteristics. This happens for example when the users write compound words such as \"hard drive\". Since this pair of words happen quite often in review for hard drives, our pipeline mistakenly detects it as an interesting collocation.\n",
    "\n",
    "## Filtering the results further\n",
    "\n",
    "We propose the following simple solution to filter out words: *If the bigram can be found in an english dictionnary, then it is not an interesting collocation.*\n",
    "\n",
    "This will for example filter out results such as \"hard drive\" or \"optical mouse\".\n",
    "\n",
    "However, findind a list of modern english words is not an easy task. Almost every dataset that can be found on the internet does not include nouns relevant for our work (such as hard drive or optical mouse). There are some online APIs such as [Oxford Dictionnary API](https://developer.oxforddictionaries.com/) or [Words API](https://www.wordsapi.com/) which return very rich results but they are priced prohibitively high, in the hundreds of dollars for the use we would make of them.\n",
    "\n",
    "But here comes Wikipedia to the rescue, or more precisely, Wiktionnary. Wiktionnary is a free to use and open online dictionnary with over five millions english words. The best part is that it contains all the words we are looking for and it can be [downloaded](https://dumps.wikimedia.org/enwiktionary/) in its entirety (just like Wikipedia). We selected the subset consisting only of the title of the wiktionnary pages, as they are named according the word for which they provide a definition. \n",
    "\n",
    "Because we only need compound words, we will be able to greatly reduce the size of the wiktionary dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376016\n"
     ]
    }
   ],
   "source": [
    "wiktionnary = pd.DataFrame.from_csv('data/enwiktionary-latest-all-titles', sep='\\t')\n",
    "\n",
    "# Keep only compound words, spaces are represented using underscores\n",
    "wiktionnary = wiktionnary[wiktionnary.page_title.str.contains('_', na=False)]\n",
    "\n",
    "# Remove garbage pages entered by the user by keeping only alphanumeric titles\n",
    "wiktionnary_filtered = wiktionnary[wiktionnary.page_title.str.contains(\"^\\w+$\", na=False, regex=True)]\n",
    "\n",
    "compound_words = wiktionnary_filtered.page_title\n",
    "compound_words = compound_words.str.lower()\n",
    "compound_words.head()\n",
    "print(len(compound_words))\n",
    "\n",
    "compound_words.to_pickle(\"data/filtered_compound_words.pickle.gz\", compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduced the size of our dictionnary down to ~376k entries. We can now use this dictionnary to filter out uninteresting collocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compound_dictionnary = pd.read_pickle(\"data/filtered_compound_words.pickle.gz\")\n",
    "\n",
    "def is_compound_word(word):\n",
    "    return word in compound_dictionnary.values\n",
    "\n",
    "def filter_bigram_list(tuple_list):\n",
    "    l = [(tup,num) for (tup,num) in tuple_list if not is_compound_word(tup[0]+'_'+tup[1])]\n",
    "    return l if l else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_product_filtered = df_product.copy()\n",
    "\n",
    "# Filter out bigrams that corresponds to words in dictionnary\n",
    "df_product_filtered.reviewText = df_product_filtered.reviewText.apply(filter_bigram_list)\n",
    "df_product_filtered = df_product_filtered[df_product_filtered.reviewText.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally display our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0528881469</td>\n",
       "      <td>[((basic, garmin), 7.05)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0972683275</td>\n",
       "      <td>[((retail, stores), 10.37), ((full, motion), 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1400501466</td>\n",
       "      <td>[((ad-supported, versions), 11.67), ((angry, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1400501776</td>\n",
       "      <td>[((full, android), 7.68)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1400532620</td>\n",
       "      <td>[((visual, feedback), 12.31), ((few, hours), 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1400532655</td>\n",
       "      <td>[((reasonably, priced), 12.12), ((angry, birds...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>140053271X</td>\n",
       "      <td>[((previously, owned), 10.8)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1400532736</td>\n",
       "      <td>[((was, able), 7.77)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9575871979</td>\n",
       "      <td>[((full, charge), 6.97), ((tactical, flashligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>9983891212</td>\n",
       "      <td>[((flat, screen), 8.94), ((n't, tested), 7.52)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>B000001OL6</td>\n",
       "      <td>[((normal, bias), 7.64), ((high, bias), 7.23)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>B00000DM9W</td>\n",
       "      <td>[((going, strong), 10.57), ((local, tv), 8.32)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>B00000J061</td>\n",
       "      <td>[((internal, ferrite), 9.98)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>B00000J1SC</td>\n",
       "      <td>[((get, free), 8.0), ((different, colors), 7.67)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>B00000J1U8</td>\n",
       "      <td>[((good, quality), 6.26)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>B00000J1V3</td>\n",
       "      <td>[((high, quality), 7.17)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>B00000J1V5</td>\n",
       "      <td>[((sharp, bends), 10.51), ((am, happy), 8.8), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>B00000J434</td>\n",
       "      <td>[((special, edition), 10.33)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>B00000J4FS</td>\n",
       "      <td>[((additional, memory), 8.56)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>B00000J4L8</td>\n",
       "      <td>[((red, hat), 8.47)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>B00000J579</td>\n",
       "      <td>[((creative, labs), 7.38)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>B00000J6WY</td>\n",
       "      <td>[((other, hand), 7.66)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>B00000JBHP</td>\n",
       "      <td>[((is, good), 4.73)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>B00000JBJQ</td>\n",
       "      <td>[((creative, labs), 7.61)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>B00000JBUI</td>\n",
       "      <td>[((ever, made), 8.05)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>B00000JD34</td>\n",
       "      <td>[((worked, great), 8.05)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>B00000JD3O</td>\n",
       "      <td>[((elastic, top), 6.39)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>B00000JD4V</td>\n",
       "      <td>[((strong, stations), 7.16)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>B00000JDF6</td>\n",
       "      <td>[((was, able), 6.85), ((well, made), 6.53)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>B00000JDHV</td>\n",
       "      <td>[((low, light), 7.09)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62922</th>\n",
       "      <td>B00JZM7TKI</td>\n",
       "      <td>[((other, cases), 6.84)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62928</th>\n",
       "      <td>B00K2MASE4</td>\n",
       "      <td>[((dual, intelligent), 9.01), ((low, voltage),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62934</th>\n",
       "      <td>B00K4PALMI</td>\n",
       "      <td>[((15.6-inch, touchscreen), 8.96)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62936</th>\n",
       "      <td>B00K4PAPG0</td>\n",
       "      <td>[((full, settings), 9.51)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62942</th>\n",
       "      <td>B00K6B7KJM</td>\n",
       "      <td>[((whole, hand), 7.55)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62943</th>\n",
       "      <td>B00K6P6MRY</td>\n",
       "      <td>[((low, price), 6.13)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62944</th>\n",
       "      <td>B00K6ZAKCW</td>\n",
       "      <td>[((external, mic), 8.4), ((external, microphon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62947</th>\n",
       "      <td>B00K7O2DJU</td>\n",
       "      <td>[((white, balance), 10.5)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62949</th>\n",
       "      <td>B00K8942SO</td>\n",
       "      <td>[((low, light), 8.09), ((24-70, f/2.8l), 7.56)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62950</th>\n",
       "      <td>B00K899B9Y</td>\n",
       "      <td>[((wide, angle), 6.83)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62954</th>\n",
       "      <td>B00K9S353M</td>\n",
       "      <td>[((same, time), 7.51)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62959</th>\n",
       "      <td>B00KC7VY3S</td>\n",
       "      <td>[((free, mini-subscriptions), 10.43), ((many, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62961</th>\n",
       "      <td>B00KCRYIWM</td>\n",
       "      <td>[((old, speakers), 5.54)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62963</th>\n",
       "      <td>B00KD5RUN2</td>\n",
       "      <td>[((internal, storage), 8.69)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62964</th>\n",
       "      <td>B00KDIT95G</td>\n",
       "      <td>[((little, bit), 8.35)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62970</th>\n",
       "      <td>B00KHA2DQM</td>\n",
       "      <td>[((many, applications), 11.27), ((idle, state)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62973</th>\n",
       "      <td>B00KHR4ZL6</td>\n",
       "      <td>[((being, able), 9.18)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62974</th>\n",
       "      <td>B00KIMX4EY</td>\n",
       "      <td>[((magnetic, connection), 8.74)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62977</th>\n",
       "      <td>B00KJJW36G</td>\n",
       "      <td>[((vertical, position), 10.71)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62981</th>\n",
       "      <td>B00KMQ3S2E</td>\n",
       "      <td>[((same, time), 7.88), ((electrical, cord), 7....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62982</th>\n",
       "      <td>B00KMRGB7C</td>\n",
       "      <td>[((other, chargers), 6.99)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62985</th>\n",
       "      <td>B00KNM763E</td>\n",
       "      <td>[((wide, angle), 10.27), ((few, minutes), 9.27)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62987</th>\n",
       "      <td>B00KOHQU58</td>\n",
       "      <td>[((non-skid, rubber), 9.4)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62989</th>\n",
       "      <td>B00KONCDVM</td>\n",
       "      <td>[((different, size), 8.25)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62991</th>\n",
       "      <td>B00KSLCU72</td>\n",
       "      <td>[((same, time), 6.3)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62993</th>\n",
       "      <td>B00KWHMR6G</td>\n",
       "      <td>[((working, great), 8.6)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62996</th>\n",
       "      <td>B00L21HC7A</td>\n",
       "      <td>[((was, able), 7.13)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62997</th>\n",
       "      <td>B00L2442H0</td>\n",
       "      <td>[((small, size), 7.03), ((external, power), 6....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62998</th>\n",
       "      <td>B00L26YDA4</td>\n",
       "      <td>[((portable, battery), 9.49), ((little, gadget...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62999</th>\n",
       "      <td>B00L3YHF6O</td>\n",
       "      <td>[((separate, amplifiers), 10.76)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15367 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             asin                                         reviewText\n",
       "0      0528881469                          [((basic, garmin), 7.05)]\n",
       "3      0972683275  [((retail, stores), 10.37), ((full, motion), 1...\n",
       "4      1400501466  [((ad-supported, versions), 11.67), ((angry, b...\n",
       "6      1400501776                          [((full, android), 7.68)]\n",
       "7      1400532620  [((visual, feedback), 12.31), ((few, hours), 9...\n",
       "8      1400532655  [((reasonably, priced), 12.12), ((angry, birds...\n",
       "9      140053271X                      [((previously, owned), 10.8)]\n",
       "10     1400532736                              [((was, able), 7.77)]\n",
       "28     9575871979  [((full, charge), 6.97), ((tactical, flashligh...\n",
       "39     9983891212    [((flat, screen), 8.94), ((n't, tested), 7.52)]\n",
       "44     B000001OL6     [((normal, bias), 7.64), ((high, bias), 7.23)]\n",
       "53     B00000DM9W    [((going, strong), 10.57), ((local, tv), 8.32)]\n",
       "60     B00000J061                      [((internal, ferrite), 9.98)]\n",
       "76     B00000J1SC  [((get, free), 8.0), ((different, colors), 7.67)]\n",
       "78     B00000J1U8                          [((good, quality), 6.26)]\n",
       "83     B00000J1V3                          [((high, quality), 7.17)]\n",
       "84     B00000J1V5  [((sharp, bends), 10.51), ((am, happy), 8.8), ...\n",
       "98     B00000J434                      [((special, edition), 10.33)]\n",
       "103    B00000J4FS                     [((additional, memory), 8.56)]\n",
       "105    B00000J4L8                               [((red, hat), 8.47)]\n",
       "111    B00000J579                         [((creative, labs), 7.38)]\n",
       "112    B00000J6WY                            [((other, hand), 7.66)]\n",
       "116    B00000JBHP                               [((is, good), 4.73)]\n",
       "118    B00000JBJQ                         [((creative, labs), 7.61)]\n",
       "121    B00000JBUI                             [((ever, made), 8.05)]\n",
       "124    B00000JD34                          [((worked, great), 8.05)]\n",
       "126    B00000JD3O                           [((elastic, top), 6.39)]\n",
       "128    B00000JD4V                       [((strong, stations), 7.16)]\n",
       "131    B00000JDF6        [((was, able), 6.85), ((well, made), 6.53)]\n",
       "135    B00000JDHV                             [((low, light), 7.09)]\n",
       "...           ...                                                ...\n",
       "62922  B00JZM7TKI                           [((other, cases), 6.84)]\n",
       "62928  B00K2MASE4  [((dual, intelligent), 9.01), ((low, voltage),...\n",
       "62934  B00K4PALMI                 [((15.6-inch, touchscreen), 8.96)]\n",
       "62936  B00K4PAPG0                         [((full, settings), 9.51)]\n",
       "62942  B00K6B7KJM                            [((whole, hand), 7.55)]\n",
       "62943  B00K6P6MRY                             [((low, price), 6.13)]\n",
       "62944  B00K6ZAKCW  [((external, mic), 8.4), ((external, microphon...\n",
       "62947  B00K7O2DJU                         [((white, balance), 10.5)]\n",
       "62949  B00K8942SO    [((low, light), 8.09), ((24-70, f/2.8l), 7.56)]\n",
       "62950  B00K899B9Y                            [((wide, angle), 6.83)]\n",
       "62954  B00K9S353M                             [((same, time), 7.51)]\n",
       "62959  B00KC7VY3S  [((free, mini-subscriptions), 10.43), ((many, ...\n",
       "62961  B00KCRYIWM                          [((old, speakers), 5.54)]\n",
       "62963  B00KD5RUN2                      [((internal, storage), 8.69)]\n",
       "62964  B00KDIT95G                            [((little, bit), 8.35)]\n",
       "62970  B00KHA2DQM  [((many, applications), 11.27), ((idle, state)...\n",
       "62973  B00KHR4ZL6                            [((being, able), 9.18)]\n",
       "62974  B00KIMX4EY                   [((magnetic, connection), 8.74)]\n",
       "62977  B00KJJW36G                    [((vertical, position), 10.71)]\n",
       "62981  B00KMQ3S2E  [((same, time), 7.88), ((electrical, cord), 7....\n",
       "62982  B00KMRGB7C                        [((other, chargers), 6.99)]\n",
       "62985  B00KNM763E   [((wide, angle), 10.27), ((few, minutes), 9.27)]\n",
       "62987  B00KOHQU58                        [((non-skid, rubber), 9.4)]\n",
       "62989  B00KONCDVM                        [((different, size), 8.25)]\n",
       "62991  B00KSLCU72                              [((same, time), 6.3)]\n",
       "62993  B00KWHMR6G                          [((working, great), 8.6)]\n",
       "62996  B00L21HC7A                              [((was, able), 7.13)]\n",
       "62997  B00L2442H0  [((small, size), 7.03), ((external, power), 6....\n",
       "62998  B00L26YDA4  [((portable, battery), 9.49), ((little, gadget...\n",
       "62999  B00L3YHF6O                  [((separate, amplifiers), 10.76)]\n",
       "\n",
       "[15367 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_product_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not perfect we see that we already get interesting results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "**<a href=\"#Table-of-Contents\">Back to table of contents</a>**\n",
    "\n",
    "We now want to be able to categorise the opinions we extracted in the previous part of this notebook. One way of doing this is to use [SentiWordNet](http://nmis.isti.cnr.it/sebastiani/Publications/LREC06.pdf). SentiWordNet is a lexical resource for performing sentiment analysis on texts. It is base on WordNet. Fortunately, the NLTK package for python provides an interface for using SentiWordNet.\n",
    "\n",
    "We implemented a class on top of NLTK's support for SentiWordNet in order to provide a convenient interface to tokenize and classify opinions. The implementation is encapsulated in the SentimentAnalyzer class in the [SentimentAnalyser.py](SentimentAnalyser.py) file.\n",
    "\n",
    "### Classifying opinions\n",
    "\n",
    "SentiWordNet provides a positivity score for each word which can then be used to asses the positivity of a sentence of for our pipeline, a pair of words. A score above than 0 denotes a positive connotation of the word while a score below 0 denotes a negative connotation. See the following examples: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from SentimentAnalyser import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentimentAnalyzer = SentimentAnalyser()\n",
    "\n",
    "examples = ['Hello world', 'The screen is poorly assembled', 'Worst purchase I\\'ve done', 'Excellent quality', 'Best deal']\n",
    "\n",
    "for sentence in examples:\n",
    "    print(\"{} : {}\".format(sentence, sentimentAnalyzer.sentiment_score_for_raw_sentence(sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using those score, we would then classify the second and the third as being negative ans the fourth and fifth as being positive. While looking very promising this approach has a major flaw for our application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "examples = ['Cheap quality', 'Low price']\n",
    "\n",
    "for sentence in examples:\n",
    "    print(\"{} : {}\".format(sentence, sentimentAnalyzer.sentiment_score_for_raw_sentence(sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not able to classify correctly opinions which are positive but writte with negatively connoted words and vice versa. This is why we propose an alternative approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis, revisited\n",
    "**<a href=\"#Table-of-Contents\">Back to table of contents</a>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we didn't get as much good result as we wished, we created a machine learning model which would be able to predict the sentiment associated with n-grams collocations. We set the negative sentiment to -1 and the positive to 1. We only keep the reviews that are smaller that 100 characters for the train set, to have for the moment a model which is fast to train (but still fairly good for our analysis). We divided the train the dataset into entries with an overall score smaller to three (bad sentiment -1) and entries with an overall score higher than 3 (good sentiment 1). We then used a multilayer perceptron classifier to classify our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.base import TransformerMixin\n",
    "np.random.seed(42)\n",
    "\n",
    "# Keep only the word with a length lower than 100\n",
    "df_ML = df_elec[(df_elec[\"reviewText\"].str.len()<100)].copy()\n",
    "# Map every overall to a score {-1,1}\n",
    "df_ML['score'] = df_elec[\"overall\"].apply(lambda x : -1 if x < 3 else 1)\n",
    "\n",
    "# Equilibrate the train data between the positive and negative sentiment\n",
    "df_0 = (df_ML[df_ML[\"score\"] == -1])\n",
    "df_1 = (df_ML[df_ML[\"score\"] == 1])\n",
    "\n",
    "if df_0.shape[0] > df_1.shape[0]:\n",
    "    df_0 = df_0.sample(df_1.shape[0])\n",
    "else:\n",
    "    df_1 = df_1.sample(df_0.shape[0])\n",
    "    \n",
    "sentiment_data = pd.concat([df_0, df_1])\n",
    "X = sentiment_data[\"reviewText\"]\n",
    "y = sentiment_data[\"score\"]\n",
    "\n",
    "# Split the data between test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our model, we will define a pipeline. This pipeline will first preprocess the data by converting the text to lower case. Then, it will vectorize the data by counting the bigrams and words present in the text. Finally it will assign those as features for the classifier. \n",
    "\n",
    "We will use the data created this way for our MLP classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the pipeline used to create the model\n",
    "bigram_clf = Pipeline([\n",
    "    (\n",
    "        'vectorizer', CountVectorizer(\n",
    "            analyzer=\"word\",\n",
    "            ngram_range=(1, 2),\n",
    "            tokenizer=word_tokenize, \n",
    "            preprocessor=lambda text: text.lower()\n",
    "        )\n",
    "    ),\n",
    "    (\n",
    "        'classifier', MLPClassifier(verbose = True)\n",
    "    )\n",
    "])\n",
    "\n",
    "bigram_clf.fit(X_train, y_train)\n",
    "bigram_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for some common entries, we get the result that we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = [\"LOW PRICE\",\"expensive\",\"cheap\",\"high quality\",\"low quality\",\"well made\",'poorly made','good product']\n",
    "y = [1,-1,-1,1,-1,1,-1,1]\n",
    "\n",
    "print(bigram_clf.score(X,y))\n",
    "print(bigram_clf.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application to brand names\n",
    "\n",
    "**<a href=\"#Table-of-Contents\">Back to table of contents</a>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can now assign a score to each product with respect to their reviews. We will use a simple rule: we will first scale the pmi score of each bigram by it's sentiment score (1 or -1); then we will simply average the scaled scores of the reviews to get the product score. We can get the score associated to the brand by avering each score of it's product.\n",
    "\n",
    "One additional step that can be taken is to regroup and aggregate the characteristics of the products by brands. This way we can see if certain brands are systematically associated with certain opinion on their products.\n",
    "\n",
    "To perform this, we will define a very simple scoring mechanism. We count the number of times each bigram appears for a certain brand and if this number reaches a certain threshold we attribute it to the brand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isInDescriptionOrTitle(text,description,title):\n",
    "    \"\"\"Filter the characteristics that are present in the name or the description of the product. This will ensure that the\n",
    "    words exctracted are only sentiment related characteristics and not some attribute of the object. This can be applied for\n",
    "    the word 'hard drive'.\n",
    "    \"\"\"\n",
    "    res = [] \n",
    "    for t in text:\n",
    "        ((a,b),num) = t\n",
    "        if (a + ' ' + b).lower() not in str(description).lower() and (a + ' ' + b).lower() not in str(title).lower():\n",
    "            res.append(t)\n",
    "    if len(res) > 0:\n",
    "        return res\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def getScore(text):\n",
    "    \"\"\"Takes the different characteristics of a product. Return the score by taking the averaged sum of the sentiment scores \n",
    "    of each characteristics.\n",
    "    \"\"\"\n",
    "    if len(text) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    res = 0.0    \n",
    "    for t in text:\n",
    "        ((a,b),num) = t\n",
    "        res += bigram_clf.predict([a + ' ' + b]) * float(num)\n",
    "    return res/len(text)\n",
    "\n",
    "def getText(text):\n",
    "    \"\"\"Return the characteristics as a string separated by ' / ' for each characteristics.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for t in text:\n",
    "        ((a,b),num) = t\n",
    "        res.append((a + ' ' + b).lower())\n",
    "    return \" / \".join(res)\n",
    "\n",
    "def getUniqueWords(text):\n",
    "    \"\"\"Return the unique characteristics from the original string text as a string separated by ' / ' for each characteristics.\n",
    "    \"\"\"\n",
    "    temp = text.split(' / ')\n",
    "    res = set(temp)\n",
    "    return \" / \".join(res)\n",
    "\n",
    "def getCharacteristics(bigrams,threshold = 2):\n",
    "    \"\"\"Return the characteristics from the list of bigrams that appear more than threshold.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for t in bigrams:\n",
    "        ((a,b),num) = t\n",
    "        res.append((a + ' ' + b).lower())\n",
    "        \n",
    "    res = list(set([i for i in res if res.count(i) >= threshold]))\n",
    "    return \" / \".join(res) if len(res) > 0 else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_final = df_product_filtered.merge(df_meta, left_on='asin', right_on='asin', how='inner')\n",
    "df_final = df_final[(df_final['brand'] != 'Unknown') & (df_final['brand'] != '')]\n",
    "\n",
    "# Filter collocations which are in title or product description\n",
    "df_final['reviewText'] = df_final.apply(lambda x : isInDescriptionOrTitle(x['reviewText'],x['description'],x['title']), axis=1)\n",
    "df_final = df_final[pd.notnull(df_final['reviewText'])]\n",
    "df_final['brand'] = df_final['brand'].str.title()\n",
    "\n",
    "display(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract the characteristics name\n",
    "df_final[\"word\"] = df_final.apply(lambda x : getText(x['reviewText']), axis=1)\n",
    "\n",
    "# Compute the score\n",
    "df_final['score'] = 0\n",
    "df_final['score'] =  df_final.apply(lambda x : getScore(x[\"reviewText\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(df_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brand rankings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally to get the scores of the different brand we will sum each scores and take the mean to have respectively the score and the mean score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group by brand\n",
    "f = {'score':['sum','mean'],'word': lambda x: ' / '.join(set(x))}\n",
    "df_brand_rank = df_final[['brand','word','score']].groupby(['brand']).agg(f)\n",
    "df_brand_rank.columns = df_brand_rank.columns.droplevel()\n",
    "df_brand_rank = df_brand_rank.reset_index()\n",
    "\n",
    "#Rename columns\n",
    "df_brand_rank = df_brand_rank.rename(columns={'sum': 'score','mean': 'mean_score', '<lambda>': 'words'})\n",
    "\n",
    "# Make sure to have unique characteristics and existing brand\n",
    "df_brand_rank['words'] = df_brand_rank.apply(lambda x : getUniqueWords(x['words']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(df_brand_rank.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the different mean scores of the brands: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = pyplot.subplots(figsize=(15,18))\n",
    "sns.barplot(y=\"brand\",x='mean_score', ax = ax,orient='h',data=df_brand_rank.sample(100).sort_values('mean_score',ascending=False),palette = sns.color_palette(\"BuGn_r\", 1));\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brand characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group by brand\n",
    "f = {'reviewText':'sum'}\n",
    "df_brand_characteristics = df_final[['brand','reviewText']].groupby(['brand']).agg(f)\n",
    "df_brand_characteristics = df_brand_characteristics.reset_index()\n",
    "\n",
    "# Make sure to have unique characteristics and existing brand\n",
    "df_brand_characteristics['characteristics'] = df_brand_characteristics.apply(lambda x : getCharacteristics(x['reviewText']), axis=1)\n",
    "df_brand_characteristics = df_brand_characteristics[pd.notnull(df_brand_characteristics['characteristics'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(df_brand_characteristics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan for milestone 3\n",
    "\n",
    "By the next milestone, we will have improved several aspects of the analysis:\n",
    "\n",
    "* We will improve the bigram analysis: extend it to trigram and compare n-gram scoring functions to see which one gives the best results for our data\n",
    "* We will parallelize the pipeline, at least the characteristics extraction as it is currently the bottleneck of our analysis, to be able to run faster on the full Electronics review dataset\n",
    "* We will finish the implementation of the brand ratings.\n",
    "\n",
    "## Data story\n",
    "\n",
    "We've opted for the data story and the idea is as follows:\n",
    "\n",
    "We want to put into practice the idea stated in the introduction, namely to complement user feedback on e-commerce platforms. The story would go roughly as follows:\n",
    "\n",
    "When visiting the presentation page, the user will be presented with a dummy e-commerce webpage with a product and some reviews. By scrolling down the page, the user will be able to see which collocations of the review are selected by our pipeline and how they contribute to the characterisation of the product. Finally, the user will be presented with a new dummy e-commerce webpage in which he will be able to see the full output of our processing pipeline for a few selected products. That is, characteristics of the product will be shown in addition to review and ratings. The more a characteristic is mentionned by the clients, the more it is emphasized on the webpage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
