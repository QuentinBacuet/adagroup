{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import gzip\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "import random \n",
    "from pylab import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = 15, 10\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we will explore opinion mining and sentiment analysis through the use of natural language processing. Our dataset consists of reviews of electronic products scraped form Amazon and available here: http://jmcauley.ucsd.edu/data/amazon/\n",
    "\n",
    "The objective of this analysis is to use the reviews of a product to extract meaningful concepts about it. Such concepts can then be used to help the user make faster and more informed decisions when shopping. The extracted concepts consist of pairs of words that describe a property of the product. Those pairs are either of the type adjective-noun or adverb-past participle.\n",
    "\n",
    "For example:\n",
    "\n",
    "* Adjective - Nouns: (good, quality) (low, price) (best, deal)\n",
    "* Adverb - Past Participle pairs such as (well, made) (poorly, assembled)\n",
    "\n",
    "The concepts can then be categorised into positive and negative opinions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<ol>\n",
    "<li> <a href=\"#Data-Overview\">Data Overview</a>\n",
    "    <ol>\n",
    "        <li><a href=\"#Reading-the-data\">Reading the data</a></li>\n",
    "        <li><a href=\"#Data-Overview\">Formats</a></li>\n",
    "        <li><a href=\"#Missing-values\">Missing values</a></li>\n",
    "        <li><a href=\"#Distributions\">Distributions</a></li>\n",
    "    </ol>\n",
    "</li>\n",
    "<p></p>\n",
    "<li> <a href=\"#Characteristic-Exctraction\">Characteristic exctraction</a>\n",
    "<ol>\n",
    "        <li><a href=\"#Data-Overview\">Data Overview</a></li>\n",
    "    </ol>\n",
    "</li>\n",
    "<p></p>\n",
    "<li><a href=\"#Sentiment-Analysis\">Sentiment Analysis</a>\n",
    "<ol>\n",
    "        <li><a href=\"#SentiWordNet\">SentiWordNet</a></li>\n",
    "        <li><a href=\"#Machine-Learning\">Machine Learning</a></li>\n",
    "    </ol>\n",
    "</li>\n",
    "<p></p>\n",
    "<li><a href=\"#Applications\">Applications</a>\n",
    "<ol>\n",
    "        <li><a href=\"#Brand-Rating\">Brand Rating</a></li>\n",
    "    </ol>\n",
    "</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data \n",
    "**<a href=\"#Table-of-Contents\">Table of contents</a>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "def readSerialized(path):\n",
    "    return pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_elec = readSerialized('serialized_electronics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_meta = readSerialized('metadata_electronics_serialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elec['reviewTime']  = pd.to_datetime(df_elec['reviewTime'],format='%m %d, %Y')\n",
    "df_elec.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = df_meta[pd.notnull(df_meta['brand'])]\n",
    "df_meta.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_merged = df_meta.merge(df_elec, left_on='asin', right_on='asin', how='inner')\n",
    "df_merged['reviewLength'] = df_merged['reviewText'].str.len()\n",
    "\n",
    "df_plot = df_merged.copy()\n",
    "df_plot = df_plot[pd.notnull(df_plot['price'])]\n",
    "df_plot = df_plot[pd.notnull(df_plot['reviewLength'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(df_plot[['price','reviewLength','overall']].sample(100000,random_state = 0), alpha=0.2, figsize=(15, 15), diagonal='hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(y='reviewLength', x='price', data=df_plot,scatter_kws={'s':5},line_kws={'color':'r'},order=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y='overall', x='reviewLength', data=df_plot, orient='h', showfliers=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "sns.boxplot(data= df_plot[['price']],orient='h', showfliers=False,ax=axes[0,0])\n",
    "df_plot[['price']].plot.kde(ax=axes[0,1])\n",
    "sns.boxplot(data= df_plot[['reviewLength']],orient='h', showfliers=False,ax=axes[1,0])\n",
    "df_plot[['reviewLength']].plot.kde(ax=axes[1,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.stats\n",
    "\n",
    "d = df_plot[['price']]\n",
    "ax = d.plot.kde()\n",
    "x = ax.get_children()[0]._x\n",
    "y = ax.get_children()[0]._y\n",
    "\n",
    "print(d.mean())\n",
    "\n",
    "scale = 70\n",
    "loc = -500\n",
    "\n",
    "pdf_fitted = scipy.stats.expon.pdf(x - loc, scale=scale)\n",
    "plt.plot(pdf_fitted,label= ('exp lambda = ' + str(scale)))\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.stats\n",
    "\n",
    "d = df_plot[['reviewLength']].sample(100000)\n",
    "ax = d.plot.kde()\n",
    "\n",
    "x = ax.get_children()[0]._x\n",
    "y = ax.get_children()[0]._y\n",
    "\n",
    "scale = 1000\n",
    "loc = -500\n",
    "\n",
    "pdf_fitted = scipy.stats.pareto.pdf(x,loc=loc,b = 2, scale=scale)\n",
    "plt.plot(pdf_fitted,label= 'exp')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[['price','reviewLength','overall']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characteristic Exctraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filterTags(w1,w2):\n",
    "    return (nltk.pos_tag(nltk.word_tokenize(w1))[0][1] == 'JJ' and nltk.pos_tag(nltk.word_tokenize(w2))[0][1] == 'NN') or \\\n",
    "     (nltk.pos_tag(nltk.word_tokenize(w1))[0][1] == 'RB' and nltk.pos_tag(nltk.word_tokenize(w2))[0][1] == 'VBN')\n",
    "\n",
    "def getBest(text):\n",
    "    ignored_words = nltk.corpus.stopwords.words('english')\n",
    "    word_filter = lambda w: len(w) < 3 or w.lower() in ignored_words\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    \n",
    "    finder = nltk.BigramCollocationFinder.from_words(tokens)\n",
    "    finder.apply_word_filter(word_filter)    \n",
    "    finder.apply_freq_filter(1)\n",
    "    res = finder.ngram_fd.most_common(1)\n",
    "        \n",
    "    res = [x for x in res if filterTags(x[0][0],x[0][1])]\n",
    "\n",
    "    if(len(res) > 0):\n",
    "        return res\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "df_product = df_elec.groupby([\"asin\"])['reviewText'].agg(lambda x:''.join(set(x))).reset_index()\n",
    "df_product = df_product.sample(n=10000)\n",
    "\n",
    "start = timer()\n",
    "df_product[\"reviewText\"] = df_product[\"reviewText\"].apply(lambda x: getBest(x))\n",
    "end = timer()\n",
    "print(end - start)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product = df_product.dropna(how = 'any')\n",
    "df_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "df_ML = df_elec[(df_elec[\"reviewText\"].str.len()<100)].copy()\n",
    "df_ML['score'] = df_elec[\"overall\"].apply(lambda x : -1 if x < 3 else 1)\n",
    "df_ML[\"reviewText\"] = df_ML[\"reviewText\"].apply(lambda x: x.lower())\n",
    "\n",
    "df_0 = (df_ML[df_ML[\"score\"] == -1])\n",
    "df_1 = (df_ML[df_ML[\"score\"] == 1])\n",
    "\n",
    "if df_0.shape[0] > df_1.shape[0]:\n",
    "    df_0 = df_0.sample(df_1.shape[0])\n",
    "else:\n",
    "    df_1 = df_1.sample(df_0.shape[0])\n",
    "    \n",
    "sentiment_data = pd.concat([df_0, df_1])\n",
    "X = sentiment_data[\"reviewText\"]\n",
    "y = sentiment_data[\"score\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1)\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_clf = Pipeline([\n",
    "('vectorizer', CountVectorizer(analyzer=\"word\",\n",
    "ngram_range=(1, 2),\n",
    "tokenizer=word_tokenize, \n",
    "# tokenizer=lambda text: mark_negation(word_tokenize(text)),\n",
    "preprocessor=lambda text: text.replace(\"<br />\", \" \"),)),\n",
    "('classifier', MLPClassifier(verbose = True))\n",
    "])\n",
    "bigram_clf.fit(X_train, y_train)\n",
    "bigram_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\"low price\",\"expensive\",\"cheap\",\"high quality\",\"low quality\",\"well made\",'poorly made','good product']\n",
    "y = [1,-1,-1,1,-1,1,-1,1]\n",
    "\n",
    "print(bigram_clf.score(X,y))\n",
    "print(bigram_clf.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application to the products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isInDescription(text,description,title):\n",
    "    res = [] \n",
    "    for t in text:\n",
    "        ((a,b),num) = t\n",
    "        if (a + ' ' + b).lower() not in str(description).lower() and (a + ' ' + b).lower() not in str(title).lower():\n",
    "            res.append(t)\n",
    "    if len(res) > 0:\n",
    "        return res\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def getScore(text):\n",
    "    if len(text) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    res = 0\n",
    "    for t in text:\n",
    "        ((a,b),num) = t\n",
    "        res += bigram_clf.predict([(a + ' ' + b).lower()])* int(num)\n",
    "    return res\n",
    "\n",
    "def getText(text):\n",
    "    res = ''\n",
    "    for t in text:\n",
    "        ((a,b),num) = t\n",
    "        res += (a + ' ' + b).lower()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_final = df_product.merge(df_meta, left_on='asin', right_on='asin', how='inner')\n",
    "\n",
    "df_final['reviewText'] = df_final.apply(lambda x : isInDescription(x['reviewText'],x['description'],x['title']), axis=1)\n",
    "df_final = df_final[pd.notnull(df_final['reviewText'])]\n",
    "\n",
    "df_final[\"word\"] = df_final.apply(lambda x : getText(x['reviewText']), axis=1)\n",
    "\n",
    "df_final['score'] = 0\n",
    "df_final['score'] =  df_final.apply(lambda x : getScore(x[\"reviewText\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = {'score':['sum'],'word': lambda x: ' / '.join(set(x))}\n",
    "df_brand = df_final[['brand','word','score']].groupby(['brand']).agg(f)\n",
    "df_brand.columns = df_brand.columns.droplevel()\n",
    "df_brand = df_brand.reset_index()\n",
    "df_brand = df_brand.rename(columns={'sum': 'score', '<lambda>': 'word'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_brand.sort_values('score',ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
