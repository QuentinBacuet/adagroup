{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import gzip\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(path): \n",
    "    g = gzip.open(path, 'rb') \n",
    "    for l in g: \n",
    "        yield eval(l) \n",
    "            \n",
    "def getDF(path): \n",
    "    i = 0 \n",
    "    df = {} \n",
    "    for d in parse(path): \n",
    "        df[i] = d \n",
    "        i += 1 \n",
    "        \n",
    "    return pd.DataFrame.from_dict(df, orient='index') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = getDF('data/reviews_Electronics_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle(\"serialized_electronics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"data/serialized_electronics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the length of the review\n",
    "df['length'] = df.reviewText.str.len()\n",
    "\n",
    "# Separate the helpful indicator list into two separate columns\n",
    "df[['helpful_yes','helpful_total']] = pd.DataFrame(df.helpful.values.tolist())\n",
    "df = df.drop('helpful', axis=1)\n",
    "\n",
    "# Compute explicitely the number of unhelpul reviews\n",
    "df['helpful_no'] = df.helpful_total - df.helpful_yes\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length as a function of overall rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(y='overall', x='length', data=df[df.length != 0], orient='h', showfliers=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpfulness as a function of length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[df.helpful_yes == df.helpful_yes.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_rated = df[df.helpful_total > 10000]\n",
    "plt.scatter(df_rated.helpful_no, df_rated.helpful_yes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_reviewed_id = df.asin.value_counts().index[0]\n",
    "\n",
    "sample = df[df.asin == most_reviewed_id]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lengths = df['reviewText'].str.len()\n",
    "lengths = lengths[(lengths > 0)]\n",
    "print(lengths.describe())\n",
    "sns.violinplot(lengths)\n",
    "plt.show()\n",
    "\n",
    "lengths_sorted = lengths.sort_values()\n",
    "y = np.array(list(range(len(lengths_sorted))))\n",
    "y = y / len(lengths_sorted)\n",
    "y = y[::-1]\n",
    "lengths_sorted.head()\n",
    "plt.plot(lengths_sorted, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviewText = sample.reviewText.copy()\n",
    "reviewText = reviewText.str.split()\n",
    "raw = [element for list_ in reviewText for element in list_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = TrigramCollocationFinder.from_words(raw)\n",
    "finder.apply_freq_filter(7)\n",
    "finder.nbest(trigram_measures.pmi, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8fc19b319d05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"And now for something completely different\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "text = word_tokenize(\"And now for something completely different\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fffff', 'NN')]\n",
      "[('poorly', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(word_tokenize(\"fffff\")))\n",
    "print(nltk.pos_tag(word_tokenize(\"poorly\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('RB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "good = wn.synset('good.n.01')\n",
    "good.lemmas()[0].pertainyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for synset in wn.synsets('great'):\n",
    "    print(synset.lemma_names())\n",
    "    print(synset.hyponyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = wn.synset('good.a.01')\n",
    "print(tmp.path_similarity(wn.synset('best.a.01')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentimentAnalyser:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "    def _penn_to_wn(self, tag):\n",
    "        \"\"\"\n",
    "        Convert between the PennTreebank tags to simple Wordnet tags\n",
    "        \"\"\"\n",
    "        if tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        elif tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        return None\n",
    "\n",
    "    def sentiment_for_tagged_word(self, tagged_word):\n",
    "        \"\"\"\n",
    "        Compute the score for a given tagged word.\n",
    "        The word is assumed to be tagged using the Penn Treebank Project's tags\n",
    "        Return None for irrelevant words, a tuple (positive score, negative score) otherwise\n",
    "        \"\"\"\n",
    "        word, tag = tagged_word\n",
    "        \n",
    "        wn_tag = self._penn_to_wn(tag)\n",
    "        \n",
    "        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "            return None\n",
    "        \n",
    "        lemma = self.lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        \n",
    "        if not lemma:\n",
    "            return None\n",
    "        \n",
    "        synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "        \n",
    "        if not synsets:\n",
    "            return None\n",
    "        \n",
    "        synset = synsets[0]\n",
    "        swn_synset = swn.senti_synset(synset.name())\n",
    "        \n",
    "        return swn_synset.pos_score(), swn_synset.neg_score()\n",
    "    \n",
    "    def sentiment_score_for_raw_sentence(self, raw_sentence):\n",
    "        \"\"\"\n",
    "        Compute the sum of the differences in sentiment score for each word in the sentence\n",
    "        \"\"\"\n",
    "        tagged_sentence = nltk.pos_tag(word_tokenize(raw_sentence))\n",
    "        sum_deltas = 0\n",
    "\n",
    "        for tagged_word in tagged_sentence:\n",
    "            scores = self.sentiment_for_tagged_word(tagged_word)\n",
    "\n",
    "            if scores is None:\n",
    "                continue\n",
    "                \n",
    "            pos_score, neg_score = scores\n",
    "            sum_deltas += (pos_score - neg_score)\n",
    "        \n",
    "        return sum_deltas\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = SentimentAnalyser()\n",
    "a = s.sentiment_score_for_raw_sentence(\"Certainly the best book I have ever read\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_sentence = \"Dankest object in the whole fucking world\"\n",
    "tagged_sentence = nltk.pos_tag(word_tokenize(raw_sentence))\n",
    "\n",
    "for word, tag in tagged_sentence:\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "        continue\n",
    "\n",
    "    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "    if not lemma:\n",
    "        continue\n",
    "\n",
    "    synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "    if not synsets:\n",
    "        continue\n",
    "\n",
    "    # Take the first sense, the most common\n",
    "    synset = synsets[0]\n",
    "    swn_synset = swn.senti_synset(synset.name())\n",
    "\n",
    "    print(swn_synset)\n",
    "    print(swn_synset.pos_score())\n",
    "    print(swn_synset.neg_score())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
